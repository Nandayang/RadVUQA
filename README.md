# RadVUQA-Radiological Visual Understanding and Question Answering Dataset
This is the official site of the study "Beyond the Hype: A dispassionate look at vision-language models in medical scenario"

**[Read the paper](https://arxiv.org/abs/2408.08704)**

RadVUQA is a large-scale dataset for developing and evaluating visionâ€“language models in medical imaging. It includes CT and MRI scans, segmentation masks, anatomical bounding boxes, and paired questionâ€“answer (QA) annotations.

---

## ğŸ“¦ Dataset Download

Download the full dataset (ZIP, ~6 GB) from Google Drive:

- [RadVUQA Dataset on Google Drive](https://drive.google.com/file/d/1upeZgVuQl_YEVCm7K38fqkjLjMeGj3Gh/view?usp=drive_link)


---

## ğŸ—‚ Directory Structure
```text
RadVUQA/
â”œâ”€â”€ CT_Release_V1/
â”‚ â”œâ”€â”€ images/
â”‚ â”œâ”€â”€ masks/
â”‚ â”œâ”€â”€ boxed_V2/
â”‚ â”œâ”€â”€ boxed_easy_V2/
â”‚ â”œâ”€â”€ CT_Release_V1.json
â”‚ â””â”€â”€ QA_CT_Release_V1.json
â”‚
â”œâ”€â”€ CT_Release_winAD_V1/
â”‚ â”œâ”€â”€ images/
â”‚ â”œâ”€â”€ masks/
â”‚ â”œâ”€â”€ boxed_V2/
â”‚ â”œâ”€â”€ boxed_easy_V2/
â”‚ â”œâ”€â”€ CT_Release_V1.json
â”‚ â””â”€â”€ QA_CT_Release_V1.json
â”‚
â”œâ”€â”€ MR_Release_V1/
â”‚ â”œâ”€â”€ images/
â”‚ â”œâ”€â”€ masks/
â”‚ â”œâ”€â”€ boxed_V2/
â”‚ â”œâ”€â”€ boxed_easy_V2/
â”‚ â”œâ”€â”€ MR_Release_V1.json
â”‚ â””â”€â”€ QA_MR_Release_V1.json
â”‚
â””â”€â”€ OOD_test/
â”œâ”€â”€ images/ (various imaging conditions)
â””â”€â”€ QA/ (out-of-distribution QA pairs)

- **images/**: Raw CT/MRI slices in DICOM or PNG.  
- **masks/**: Segmentation masks for anatomical structures.  
- **boxed_V2/**: Standard bounding box annotations.  
- **boxed_easy_V2/**: Simplified bounding box annotations.  
- **`*_Release_V1.json`**: Metadata files listing image IDs and target coordinates.  
- **`QA_*_Release_V1.json`**: JSON files containing questionâ€“answer pairs.  
- **OOD_test/**: Out-of-distribution test set for robustness evaluation.
```
---

## ğŸš€ Usage

1. ** images and masks**  
   Use `images/` and `masks/` folders for raw images and corresponding masks.

2. **Apply bounding boxes**  
   Use `boxed_V2/` or `boxed_easy_V2/` for images with bounding boxes.

3. **Parse QA pairs**  
   Use `QA_*_Release_V1.json` for questionâ€“answer annotations linked to each image.

4. **Utilize metadata**  
   Reference `*_Release_V1.json` for image-level metadata and anatomical coordinates.

5. **Evaluate on OOD**  
   Test model generalization on `OOD_test/` with varied imaging conditions.

---

## ğŸ“ Citation

If you use RadVUQA in your research, please cite the following papers:

1. Nan Y, Zhou H, Xing X, et al. _Beyond the Hype: A Dispassionate Look at Visionâ€“Language Models in Medical Scenario_. **IEEE Trans. Neural Networks Learn. Syst.**, 2025.

2. Wasserthal J, Breit H C, Meyer M T, et al. _TotalSegmentator: robust segmentation of 104 anatomic structures in CT images_. **Radiology: Artificial Intelligence**, 2023, 5(5): e230024.

3. Akinci Dâ€™Antonoli T, Berger L K, Indrakanti A K, et al. _TotalSegmentator MRI: Robust Sequence-independent Segmentation of Multiple Anatomic Structures in MRI_. **Radiology**, 2025, 314(2): e241613.

---

## ğŸ“– License

[CC BY 4.0]
